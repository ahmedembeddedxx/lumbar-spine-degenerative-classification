{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a48be77",
   "metadata": {},
   "source": [
    "# **A Novel Approach for Three-Way Classification of Lumbar Spine Degeneration Using Pseudo-Modality Learning to Handle Missing MRI Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b241a70",
   "metadata": {},
   "source": [
    "<img src=\"../../architecture/classifiers-architecture/mri-processor.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b4bef",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9c4543",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-15T06:28:11.387501Z",
     "iopub.status.busy": "2024-10-15T06:28:11.387038Z",
     "iopub.status.idle": "2024-10-15T06:28:17.933932Z",
     "shell.execute_reply": "2024-10-15T06:28:17.932477Z"
    },
    "papermill": {
     "duration": 6.554633,
     "end_time": "2024-10-15T06:28:17.937027",
     "exception": false,
     "start_time": "2024-10-15T06:28:11.382394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee188ec",
   "metadata": {},
   "source": [
    "### Embeddings Generator using Residual Net50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "812e8a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T06:28:17.944392Z",
     "iopub.status.busy": "2024-10-15T06:28:17.943779Z",
     "iopub.status.idle": "2024-10-15T06:28:17.962302Z",
     "shell.execute_reply": "2024-10-15T06:28:17.960739Z"
    },
    "papermill": {
     "duration": 0.025397,
     "end_time": "2024-10-15T06:28:17.965169",
     "exception": false,
     "start_time": "2024-10-15T06:28:17.939772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(dir_name, csv_path, study_path):\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results = []\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        patient_id = str(row['study_id'])\n",
    "        series_id = str(row['series_id'])\n",
    "        series_path = os.path.join(study_path, patient_id, series_id)\n",
    "        embeddings = []\n",
    "\n",
    "        for slice_file in os.listdir(series_path):\n",
    "            if slice_file.endswith('.npy'):\n",
    "                slice_path = os.path.join(series_path, slice_file)\n",
    "                slice_data = np.load(slice_path)\n",
    "\n",
    "                if slice_data.ndim == 2:\n",
    "                    slice_data = np.stack([slice_data] * 3, axis=0)\n",
    "                elif slice_data.ndim == 3 and slice_data.shape[0] == 1:\n",
    "                    slice_data = np.repeat(slice_data, 3, axis=0)\n",
    "\n",
    "                input_tensor = torch.from_numpy(slice_data).float()\n",
    "                input_tensor = transforms.Resize((224, 224))(input_tensor)\n",
    "                input_tensor = (input_tensor - torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)) / torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    embedding = model(input_tensor).view(-1)\n",
    "                    embeddings.append(embedding.numpy())\n",
    "\n",
    "        if embeddings:\n",
    "            average_embedding = np.mean(np.vstack(embeddings), axis=0)\n",
    "            embedding_dict = {f'{i}': average_embedding[i] for i in range(2048)}\n",
    "            embedding_dict.update({'study_id': patient_id, 'series_id': series_id})\n",
    "            results.append(embedding_dict)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'{dir_name}/final_embeddings.csv', index=False)\n",
    "    torch.save(model.state_dict(), f'{dir_name}/model_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2226it [43:50,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings with attention completed and saved to AT2_attention_embeddings_gsl.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2226it [43:54,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings with attention completed and saved to AT2_attention_embeddings_hist.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1876it [18:31,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings with attention completed and saved to ST2_attention_embeddings_gsl.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1876it [18:33,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings with attention completed and saved to ST2_attention_embeddings_hist.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1881it [19:11,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings with attention completed and saved to ST1_attention_embeddings_gsl.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1881it [18:52,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings with attention completed and saved to ST1_attention_embeddings_hist.csv\n"
     ]
    }
   ],
   "source": [
    "generate_embeddings(\n",
    "    dir_name='at2-greyscl', \n",
    "    csv_path='/kaggle/input/preprocessed-dataset/train_data_AT2.csv', \n",
    "    study_path='/kaggle/input/preprocessed-dataset/grey_scale_train'\n",
    ")\n",
    "\n",
    "generate_embeddings(\n",
    "    dir_name='at2-hist', \n",
    "    csv_path='/kaggle/input/preprocessed-dataset/train_data_AT2.csv', \n",
    "    study_path='/kaggle/input/preprocessed-dataset/hist_norm_train'\n",
    ")\n",
    "\n",
    "generate_embeddings(\n",
    "    dir_name='st2-greyscl', \n",
    "    csv_path='/kaggle/input/preprocessed-dataset/train_data_ST2.csv', \n",
    "    study_path='/kaggle/input/preprocessed-dataset/grey_scale_train'\n",
    ")\n",
    "\n",
    "generate_embeddings(\n",
    "    dir_name='st2-hist', \n",
    "    csv_path='/kaggle/input/preprocessed-dataset/train_data_ST2.csv', \n",
    "    study_path='/kaggle/input/preprocessed-dataset/hist_norm_train'\n",
    ")\n",
    "\n",
    "generate_embeddings(\n",
    "    dir_name='st1-greyscl', \n",
    "    csv_path='/kaggle/input/preprocessed-dataset/train_data_ST1.csv', \n",
    "    study_path='/kaggle/input/preprocessed-dataset/grey_scale_train'\n",
    ")\n",
    "\n",
    "generate_embeddings(\n",
    "    dir_name='st1-hist', \n",
    "    csv_path='/kaggle/input/preprocessed-dataset/train_data_ST1.csv', \n",
    "    study_path='/kaggle/input/preprocessed-dataset/hist_norm_train'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5788739,
     "sourceId": 9528888,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-15T06:28:08.100479",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
