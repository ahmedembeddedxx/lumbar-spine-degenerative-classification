{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9528888,"sourceType":"datasetVersion","datasetId":5788739},{"sourceId":9538829,"sourceType":"datasetVersion","datasetId":5810383},{"sourceId":9544899,"sourceType":"datasetVersion","datasetId":5814948},{"sourceId":9553895,"sourceType":"datasetVersion","datasetId":5821406}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A Novel Approach for Three-Way Classification of Lumbar Spine Degeneration Using Pseudo-Modality Learning to Handle Missing MRI Data","metadata":{}},{"cell_type":"markdown","source":"## Libs","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport random\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:05:45.740795Z","iopub.execute_input":"2024-10-07T16:05:45.741424Z","iopub.status.idle":"2024-10-07T16:05:45.746602Z","shell.execute_reply.started":"2024-10-07T16:05:45.741383Z","shell.execute_reply":"2024-10-07T16:05:45.745506Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Loading MedicalNet50 Embeddings","metadata":{}},{"cell_type":"code","source":"csv1 = pd.read_csv('/kaggle/input/medicalnet-attention-layers-for-rsna/AT2_attention_embeddings_hist.csv')\ncsv2 = pd.read_csv('/kaggle/input/medicalnet-attention-layers-for-rsna/ST1_attention_embeddings_hist.csv')\ncsv3 = pd.read_csv('/kaggle/input/medicalnet-attention-layers-for-rsna/ST2_attention_embeddings_hist.csv')\n\nmerged_data = pd.concat([csv1, csv2, csv3], ignore_index=True)\n\nunique_study_ids = merged_data['study_id'].unique()\ntrain_ids, test_ids = train_test_split(unique_study_ids, test_size=0.2, random_state=42)\n\ntrain_data = merged_data[merged_data['study_id'].isin(train_ids)]\ntest_data = merged_data[merged_data['study_id'].isin(test_ids)]\n\ndef simulate_missing_modalities(data, missing_rate=0.5):\n    grouped = data.groupby('study_id')\n    simulated_data = []\n    \n    for study_id, group in grouped:\n        num_embeddings = len(group)\n        num_to_keep = max(1, int(num_embeddings * (1 - missing_rate)))  \n        keep_indices = random.sample(range(num_embeddings), num_to_keep)\n        for idx in keep_indices:\n            simulated_data.append(group.iloc[idx])\n    \n    return pd.DataFrame(simulated_data)\n\nsimulated_test_data = simulate_missing_modalities(test_data, missing_rate=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:02:57.016871Z","iopub.execute_input":"2024-10-07T16:02:57.017298Z","iopub.status.idle":"2024-10-07T16:02:58.838664Z","shell.execute_reply.started":"2024-10-07T16:02:57.017265Z","shell.execute_reply":"2024-10-07T16:02:58.837890Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Defining Architecture","metadata":{}},{"cell_type":"code","source":"class DenoisingAutoencoder(nn.Module):\n    def __init__(self, input_dim=512, hidden_dim=256, output_dim=512):\n        super(DenoisingAutoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \nclass VAE(nn.Module):\n    def __init__(self, input_dim=512, latent_dim=128):\n        super(VAE, self).__init__()\n        \n        self.fc1 = nn.Linear(input_dim, 256)\n        self.fc21 = nn.Linear(256, latent_dim)\n        self.fc22 = nn.Linear(256, latent_dim)\n        \n        self.fc3 = nn.Linear(latent_dim, 256)\n        self.fc4 = nn.Linear(256, input_dim)\n\n    def encode(self, x):\n        h1 = F.relu(self.fc1(x))\n        return self.fc21(h1), self.fc22(h1)\n    \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def decode(self, z):\n        h3 = F.relu(self.fc3(z))\n        return torch.tanh(self.fc4(h3))\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\ndef vae_loss_function(reconstructed_x, x, mu, logvar):\n    recon_loss = F.mse_loss(reconstructed_x, x, reduction='sum')\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    \n    return recon_loss + kl_loss\n\nclass AttentionFusion(nn.Module):\n    def __init__(self, embedding_dim=512):\n        super(AttentionFusion, self).__init__()\n        self.attention_weights = nn.Parameter(torch.ones(1, embedding_dim), requires_grad=True)\n\n    def forward(self, embeddings):\n        attention_scores = torch.matmul(embeddings, self.attention_weights.T)\n        attention_weights = torch.softmax(attention_scores, dim=0)\n\n        fused_embedding = torch.sum(attention_weights * embeddings, dim=0)\n        return fused_embedding","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:02:58.839807Z","iopub.execute_input":"2024-10-07T16:02:58.840131Z","iopub.status.idle":"2024-10-07T16:02:58.855183Z","shell.execute_reply.started":"2024-10-07T16:02:58.840099Z","shell.execute_reply":"2024-10-07T16:02:58.854221Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Training Functions","metadata":{}},{"cell_type":"code","source":"def train_denoising_autoencoder(autoencoder, optimizer, train_data, num_epochs=50, noise_factor=0.2):\n    autoencoder.train()\n    grouped = train_data.groupby('study_id')\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    autoencoder.to(device)\n\n    mse_loss_fn = nn.MSELoss()\n\n    for epoch in tqdm(range(num_epochs)):\n        total_loss = 0\n        for study_id, group in (grouped):\n\n            embeddings = []\n            for _, row in group.iterrows():\n                embedding = row[0:512].to_numpy(dtype=float)  \n                embeddings.append(embedding)\n\n            embeddings = np.array(embeddings)  \n\n            num_embeddings = embeddings.shape[0]\n\n            if num_embeddings > 1:\n                input_embeddings = embeddings[0]\n                noisy_embeddings = input_embeddings + noise_factor * np.random.randn(*input_embeddings.shape)  # Add noise\n                noisy_embeddings = np.clip(noisy_embeddings, 0., 1.)\n                \n                input_embeddings = torch.tensor(input_embeddings, dtype=torch.float32).to(device)\n                noisy_embeddings = torch.tensor(noisy_embeddings, dtype=torch.float32).to(device)\n\n                reconstructed_embeddings = autoencoder(noisy_embeddings)\n\n                loss = mse_loss_fn(reconstructed_embeddings, input_embeddings)\n                total_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n    print(f'Total Loss: {total_loss/len(grouped)}')\n        \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_vae(vae, optimizer, train_data, num_epochs=50):\n    vae.train()\n    grouped = train_data.groupby('study_id')\n    \n    for epoch in tqdm(range(num_epochs)):\n        total_loss = 0\n        for study_id, group in (grouped):\n\n            embeddings = []\n            for _, row in group.iterrows():\n                embedding = row[0:512].to_numpy(dtype=float)  \n                embeddings.append(embedding)\n\n            embeddings = np.array(embeddings)\n\n            if len(embeddings) > 1:\n                input_embedding = embeddings[0]\n                noisy_embedding = input_embedding + 0.3 * np.random.randn(*input_embedding.shape)  # Add noise\n\n                input_embedding = torch.tensor(input_embedding, dtype=torch.float32).to(device)\n                noisy_embedding = torch.tensor(noisy_embedding, dtype=torch.float32).to(device)\n\n                optimizer.zero_grad()\n\n                reconstructed_embedding, mu, logvar = vae(noisy_embedding)\n\n                loss = vae_loss_function(reconstructed_embedding, input_embedding, mu, logvar)\n\n                total_loss += loss.item()\n\n                loss.backward()\n                optimizer.step()\n\n    print(f'Total Loss: {total_loss/len(grouped)}')\n\nfusion_layer = AttentionFusion(embedding_dim=512).to(device)\n\ndef train_with_attention_fusion(vae, optimizer, train_data, num_epochs=50):\n    vae.train()\n    fusion_layer.train()\n    grouped = train_data.groupby('study_id')\n    \n    for epoch in tqdm(range(num_epochs)):\n        total_loss = 0\n        for study_id, group in (grouped):\n            embeddings = []\n            for _, row in group.iterrows():\n                embedding = row[0:512].to_numpy(dtype=float)  \n                embeddings.append(embedding)\n\n            embeddings = np.array(embeddings)\n\n            if len(embeddings) > 1:\n                embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n                \n                fused_embedding = fusion_layer(embeddings_tensor)\n\n                noisy_embedding = fused_embedding + 0.3 * torch.randn(*fused_embedding.shape).to(device)\n\n                optimizer.zero_grad()\n                reconstructed_embedding, mu, logvar = vae(noisy_embedding)\n\n                loss = vae_loss_function(reconstructed_embedding, fused_embedding, mu, logvar)\n\n                total_loss += loss.item()\n\n                loss.backward()\n                optimizer.step()\n\n    print(f'Total Loss: {total_loss/len(grouped)}')","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:06:16.421608Z","iopub.execute_input":"2024-10-07T16:06:16.422215Z","iopub.status.idle":"2024-10-07T16:06:16.445219Z","shell.execute_reply.started":"2024-10-07T16:06:16.422177Z","shell.execute_reply":"2024-10-07T16:06:16.444385Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"autoencoder = DenoisingAutoencoder(input_dim=512, hidden_dim=256, output_dim=512)\noptimizer = optim.Adam(autoencoder.parameters(), lr=0.0001)\n\ntrain_denoising_autoencoder(autoencoder, optimizer, train_data, num_epochs=20)\n\nvae = VAE(input_dim=512, latent_dim=256).to(device)\noptimizer = optim.Adam(vae.parameters(), lr=0.001)\n\ntrain_vae(vae, optimizer, train_data, num_epochs=20)\n\nattention_fusion = VAE(input_dim=512, latent_dim=128).to(device)\noptimizer = optim.Adam(vae.parameters(), lr=0.001)\n\ntrain_with_attention_fusion(attention_fusion, optimizer, train_data, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:06:21.964392Z","iopub.execute_input":"2024-10-07T16:06:21.965153Z","iopub.status.idle":"2024-10-07T16:10:30.200682Z","shell.execute_reply.started":"2024-10-07T16:06:21.965110Z","shell.execute_reply":"2024-10-07T16:10:30.199819Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 20/20 [01:10<00:00,  3.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Total Loss: 52.91559053136459\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [01:28<00:00,  4.44s/it]\n","output_type":"stream"},{"name":"stdout","text":"Total Loss: 27095.971876089767\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [01:29<00:00,  4.46s/it]","output_type":"stream"},{"name":"stdout","text":"Total Loss: 13440.086152139622\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Storing Models","metadata":{}},{"cell_type":"code","source":"torch.save(autoencoder.state_dict(), 'autoencoder.pth')\ntorch.save(vae.state_dict(), 'vae.pth')\ntorch.save(attention_fusion.state_dict(), 'attention_fusion.pth')","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:11:40.982817Z","iopub.execute_input":"2024-10-07T16:11:40.983214Z","iopub.status.idle":"2024-10-07T16:11:41.001619Z","shell.execute_reply.started":"2024-10-07T16:11:40.983176Z","shell.execute_reply":"2024-10-07T16:11:41.000839Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"autoencoder.load_state_dict(torch.load('autoencoder.pth', weights_only=True))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nautoencoder.to(device)\n\nvae.load_state_dict(torch.load('vae.pth', weights_only=True))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvae.to(device)\n\nattention_fusion.load_state_dict(torch.load('attention_fusion.pth', weights_only=True))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nattention_fusion.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:14:43.105595Z","iopub.execute_input":"2024-10-07T16:14:43.106351Z","iopub.status.idle":"2024-10-07T16:14:43.130504Z","shell.execute_reply.started":"2024-10-07T16:14:43.106308Z","shell.execute_reply":"2024-10-07T16:14:43.129656Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"VAE(\n  (fc1): Linear(in_features=512, out_features=256, bias=True)\n  (fc21): Linear(in_features=256, out_features=128, bias=True)\n  (fc22): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=256, bias=True)\n  (fc4): Linear(in_features=256, out_features=512, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluation Pipeline","metadata":{}},{"cell_type":"code","source":"def evaluate_with_cosine_distance(autoencoder, test_data):\n    autoencoder.eval()\n    grouped = test_data.groupby('study_id')\n    \n    mse_scores = []\n    cosine_distances = []\n    \n    with torch.no_grad():\n        for study_id, group in tqdm(grouped):\n            embeddings = [row[:512].to_numpy(dtype=float) for _, row in group.iterrows()]\n            embeddings = np.array(embeddings)\n\n            if len(embeddings) > 1:\n                input_embedding = embeddings[0] \n                noisy_embedding = input_embedding + 0.3 * np.random.randn(*input_embedding.shape)\n                \n                input_embedding = torch.tensor(input_embedding, dtype=torch.float32).to(device)\n                noisy_embedding = torch.tensor(noisy_embedding, dtype=torch.float32).to(device)\n\n                generated_embedding = autoencoder(noisy_embedding)\n\n                mse = F.mse_loss(input_embedding, generated_embedding).item()\n                mse_scores.append(mse)\n\n                cosine_similarity = F.cosine_similarity(input_embedding.unsqueeze(0), generated_embedding.unsqueeze(0)).item()\n                cosine_distance = 1 - cosine_similarity\n                cosine_distances.append(cosine_distance)\n\n    print(f'Average MSE on Unseen Data: {np.mean(mse_scores)}')\n    print(f'Average Cosine Distance on Unseen Data: {np.mean(cosine_distances)}')\n    \n    \ndef evaluate_vae_with_cosine_distance(vae, test_data):\n    vae.eval()\n    grouped = test_data.groupby('study_id')\n    \n    mse_scores = []\n    cosine_distances = []\n\n    with torch.no_grad():\n        for study_id, group in tqdm(grouped):\n            embeddings = [row[:512].to_numpy(dtype=float) for _, row in group.iterrows()]\n            embeddings = np.array(embeddings)\n\n            if len(embeddings) > 1:\n                input_embedding = embeddings[0]\n                noisy_embedding = input_embedding + 0.3 * np.random.randn(*input_embedding.shape)  # Add noise\n\n                input_embedding = torch.tensor(input_embedding, dtype=torch.float32).to(device)\n                noisy_embedding = torch.tensor(noisy_embedding, dtype=torch.float32).to(device)\n\n                reconstructed_embedding, _, _ = vae(noisy_embedding)\n\n                mse = F.mse_loss(input_embedding, reconstructed_embedding).item()\n                mse_scores.append(mse)\n\n                cosine_similarity = F.cosine_similarity(input_embedding.unsqueeze(0), reconstructed_embedding.unsqueeze(0)).item()\n                cosine_distance = 1 - cosine_similarity\n                cosine_distances.append(cosine_distance)\n\n    print(f'Average MSE on Unseen Data: {np.mean(mse_scores)}')\n    print(f'Average Cosine Distance on Unseen Data: {np.mean(cosine_distances)}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:15:08.720280Z","iopub.execute_input":"2024-10-07T16:15:08.720900Z","iopub.status.idle":"2024-10-07T16:15:08.735658Z","shell.execute_reply.started":"2024-10-07T16:15:08.720859Z","shell.execute_reply":"2024-10-07T16:15:08.734567Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Unseen Data Metrics","metadata":{}},{"cell_type":"code","source":"evaluate_with_cosine_distance(autoencoder, simulated_test_data)    \nevaluate_vae_with_cosine_distance(vae, simulated_test_data)\nevaluate_vae_with_cosine_distance(attention_fusion, simulated_test_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:16:03.912279Z","iopub.execute_input":"2024-10-07T16:16:03.912664Z","iopub.status.idle":"2024-10-07T16:16:04.248364Z","shell.execute_reply.started":"2024-10-07T16:16:03.912628Z","shell.execute_reply":"2024-10-07T16:16:04.247359Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"100%|██████████| 376/376 [00:00<00:00, 3875.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average MSE on Unseen Data: 24.237207994378846\nAverage Cosine Distance on Unseen Data: 0.6346563293502249\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 376/376 [00:00<00:00, 3578.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average MSE on Unseen Data: 24.343631791657415\nAverage Cosine Distance on Unseen Data: 0.6387393695848256\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 376/376 [00:00<00:00, 3699.71it/s]","output_type":"stream"},{"name":"stdout","text":"Average MSE on Unseen Data: 27.50167006254196\nAverage Cosine Distance on Unseen Data: 0.9744737282363248\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}